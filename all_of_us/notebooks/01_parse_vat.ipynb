{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import tqdm\n",
    "import os\n",
    "import sysf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse VAT using dsub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## First pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "USER_NAME = os.getenv('OWNER_EMAIL').split('@')[0].replace('.','-')\n",
    "%env USER_NAME={USER_NAME}\n",
    "\n",
    "\n",
    "JOB_NAME='get_vat'\n",
    "%env JOB_NAME={JOB_NAME}\n",
    "\n",
    "%env DOCKER_PREFIX=XXXX # this is censored because pulling docker images from google container registry charges the owner of the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%writefile ~/aou_dsub.bash\n",
    "\n",
    "function aou_dsub () {\n",
    "\n",
    "  # Get a shorter username to leave more characters for the job name.\n",
    "  local DSUB_USER_NAME=\"$(echo \"${OWNER_EMAIL}\" | cut -d@ -f1)\"\n",
    "\n",
    "  # For AoU RWB projects network name is \"network\".\n",
    "  local AOU_NETWORK=network\n",
    "  local AOU_SUBNETWORK=subnetwork\n",
    "\n",
    "  dsub \\\n",
    "      --provider google-cls-v2 \\\n",
    "      --user-project \"${GOOGLE_PROJECT}\"\\\n",
    "      --project \"${GOOGLE_PROJECT}\"\\\n",
    "      --boot-disk-size 40 \\\n",
    "      --network \"${AOU_NETWORK}\" \\\n",
    "      --subnetwork \"${AOU_SUBNETWORK}\" \\\n",
    "      --service-account \"$(gcloud config get-value account)\" \\\n",
    "      --user \"${DSUB_USER_NAME}\" \\\n",
    "      --regions us-central1 \\\n",
    "      --logging \"${WORKSPACE_BUCKET}/dsub/logs/{job-name}/{user-id}/$(date +'%Y%m%d/%H%M%S')/{job-id}-{task-id}-{task-attempt}.log\" \\\n",
    "      \"$@\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%writefile parse_vat.sh\n",
    "\n",
    "set -o errexit\n",
    "set -o xtrace\n",
    "\n",
    "gsutil -u ${GOOGLE_PROJECT} -m cp gs://fc-aou-datasets-controlled/v8/wgs/short_read/snpindel/aux/vat/vat_complete.bgz.tsv.gz vat_complete.bgz.tsv.gz\n",
    "\n",
    "python -c \"import pandas as pd; import numpy as np; import tqdm;\n",
    "worker_index=${WORKER_INDEX}\n",
    "WORKSPACE_BUCKET=${WORKSPACE_BUCKET}\n",
    "header = pd.read_csv('vat_complete.bgz.tsv.gz', nrows = 0, sep = '\\t', dtype = str).columns.tolist();\n",
    "transcript_specific_columns = [\n",
    "    'transcript',\n",
    "    'gene_symbol',\n",
    "    'transcript_source',\n",
    "    'aa_change',\n",
    "    'consequence',\n",
    "    'dna_change_in_transcript',\n",
    "    'exon_number',\n",
    "    'intron_number',\n",
    "    'gene_id',\n",
    "    'is_canonical_transcript',\n",
    "    \n",
    "];\n",
    "relevant_columns = ['vid',\n",
    " 'transcript',\n",
    " 'contig',\n",
    " 'position',\n",
    " 'ref_allele',\n",
    " 'alt_allele',\n",
    " 'gvs_afr_ac',\n",
    " 'gvs_afr_an',\n",
    " 'gvs_afr_af',\n",
    " 'gvs_eur_ac',\n",
    " 'gvs_eur_an',\n",
    " 'gvs_eur_af',\n",
    " 'gene_symbol',\n",
    " 'transcript_source',\n",
    " 'aa_change',\n",
    " 'consequence',\n",
    " 'dna_change_in_transcript',\n",
    " 'variant_type',\n",
    " 'exon_number',\n",
    " 'intron_number',\n",
    " 'genomic_location',\n",
    " 'dbsnp_rsid',\n",
    " 'gene_id',\n",
    " 'gene_omim_id',\n",
    " 'is_canonical_transcript',\n",
    " 'revel',\n",
    " 'splice_ai_acceptor_gain_score',\n",
    " 'splice_ai_acceptor_gain_distance',\n",
    " 'splice_ai_acceptor_loss_score',\n",
    " 'splice_ai_acceptor_loss_distance',\n",
    " 'splice_ai_donor_gain_score',\n",
    " 'splice_ai_donor_gain_distance',\n",
    " 'splice_ai_donor_loss_score',\n",
    " 'splice_ai_donor_loss_distance',\n",
    " 'omim_phenotypes_id',\n",
    " 'omim_phenotypes_name',\n",
    " 'clinvar_classification',\n",
    " 'clinvar_last_updated',\n",
    " 'clinvar_phenotype'];\n",
    "relevant_column_indices = np.flatnonzero([c in relevant_columns for c in header]).tolist()\n",
    "non_specific_columns = [c for c in relevant_columns if c not in transcript_specific_columns]\n",
    "\n",
    "def compress_df_chunk(df, extra_columns = None):\n",
    "    non_specific_columns_other_than_vid = [c for c in non_specific_columns if c != 'vid'] + extra_colummns\n",
    "    non_specific_component = df.iloc[0,][non_specific_columns_other_than_vid]\n",
    "    transcript_specific_component = df[transcript_specific_columns].astype(str).agg(','.join)\n",
    "    return pd.concat([non_specific_component, transcript_specific_component])\n",
    "\n",
    "def gsutil_cp(src, dst, clobber = True, quiet = False):\n",
    "    import subprocess\n",
    "\n",
    "    gsutil_options = ['-m']\n",
    "    if quiet:\n",
    "        gsutil_options += ['-q']\n",
    "\n",
    "    cp_options = []\n",
    "    if not clobber:\n",
    "        cp_options += ['-n']\n",
    "\n",
    "    subprocess.run(\n",
    "        ['gsutil'] + gsutil_options + ['cp'] + cp_options + [src, dst],\n",
    "        check = True,\n",
    "        universal_newlines = True\n",
    "    )\n",
    "\n",
    "dtypes = {c : str for c in relevant_columns}\n",
    "dtypes['gvs_afr_ac'] = int\n",
    "dtypes['gvs_afr_an'] = int\n",
    "dtypes['gvs_afr_af'] = float \n",
    "dtypes['gvs_eur_ac'] = int\n",
    "dtypes['gvs_eur_an'] = int\n",
    "dtypes['gvs_eur_af'] = float\n",
    "vat_iterator = pd.read_csv(\n",
    "    'vat_complete.bgz.tsv.gz', \n",
    "    chunksize = int(1e6), \n",
    "    sep = '\\t', \n",
    "    header = 0,\n",
    "    names = relevant_columns,\n",
    "    dtype = dtypes,\n",
    "    usecols = relevant_column_indices,\n",
    "    nrows = int(1e9),\n",
    "    skiprows = int(1e9 * worker_index)   \n",
    ")\n",
    "n_afr = 79826\n",
    "n_eur = 223350\n",
    "\n",
    "allele_number_threshold_afr = n_afr * 2 * .95\n",
    "allele_number_threshold_eur = n_eur * 2 * .95\n",
    "\n",
    "old_chromosomes = None\n",
    "relevant_chromosomes = [f'chr{i}' for i in range(1,23)]\n",
    "for chunk in tqdm.tqdm(vat_iterator):\n",
    "    if worker_index == 4:\n",
    "        chunk = chunk.query('contig in @relevant_chromosomes')\n",
    "    chunk_filtered = chunk.assign(\n",
    "        gvs_afr_an = lambda df: df.gvs_afr_an.astype(int),\n",
    "        gvs_eur_an = lambda df: df.gvs_eur_an.astype(int),\n",
    "        gvs_afr_af = lambda df: df.gvs_afr_af.astype(float),\n",
    "        gvs_eur_af = lambda df: df.gvs_eur_af.astype(float),\n",
    "    ).query(\n",
    "        '(0.001 <= gvs_afr_af <= .999) | (0.001 <= gvs_eur_af <= .999)'\n",
    "    ).query(\n",
    "        'gvs_afr_an >= @allele_number_threshold_afr'\n",
    "    ).query(\n",
    "        'gvs_eur_an >= @allele_number_threshold_eur'\n",
    "    )\n",
    "    chunk_filtered_collapsed = chunk_filtered.groupby(\n",
    "        ['vid'], sort = False\n",
    "    ).apply(\n",
    "        compress_df_chunk\n",
    "    ).reset_index(\n",
    "    ).assign(\n",
    "        gvs_eur_missingness = lambda df: df.gvs_eur_an / (n_eur * 2),\n",
    "        gvs_afr_missingness = lambda df: df.gvs_afr_an / (n_afr * 2),\n",
    "        CHR = lambda df: df.contig.str.slice(3)\n",
    "    )\n",
    "    new_chromosomes = chunk_filtered_collapsed.CHR.drop_duplicates().tolist()\n",
    "    if old_chromosomes is None:\n",
    "        old_chromosomes = new_chromosomes\n",
    "    for chromosome in new_chromosomes:\n",
    "        chunk_filtered_collapsed.query(\n",
    "            'CHR == @chromosome'\n",
    "        ).to_csv(\n",
    "            f'vat_chr{chromosome}_worker_{worker_index}.tsv',\n",
    "            mode = 'a',\n",
    "            header = False,\n",
    "            index = False,\n",
    "            sep = '\\t'\n",
    "        )\n",
    "    for chromosome in old_chromosomes:\n",
    "        if chromosome not in new_chromosomes:\n",
    "            gsutil_cp(f'vat_chr{chromosome}_worker_{worker_index}.tsv', f'{WORKSPACE_BUCKET}/data/vat/vat_chr{chromosome}_worker_{worker_index}.tsv')\n",
    "    old_chromosomes = new_chromosomes\"\n",
    "\n",
    "gsutil -m cp vat_chr* ${WORKSPACE_BUCKET}/data/vat/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "task_df = pd.DataFrame({\n",
    "    '--env WORKER_INDEX' : [i for i in range(5)]\n",
    "})\n",
    "task_df.to_csv('index_df.tsv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%bash --out JOB_NAME\n",
    "source ~/aou_dsub.bash # This file was created via notebook 01_dsub_setup.ipynb.\n",
    "                \n",
    "aou_dsub \\\n",
    "  --machine-type n1-highmem-2 \\\n",
    "  --task index_df.tsv 2-5 \\\n",
    "  --env WORKSPACE_BUCKET ${WORKSPACE_BUCKET} \\\n",
    "  --image \"${DOCKER_PREFIX}:1.4\" \\\n",
    "  --disk-size 1000 \\\n",
    "  --script parse_vat.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## finish broken chromosome 22 separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%writefile parse_vat_22.sh\n",
    "\n",
    "set -o xtrace\n",
    "\n",
    "gsutil -u ${GOOGLE_PROJECT} -m cp gs://fc-aou-datasets-controlled/v8/wgs/short_read/snpindel/aux/vat/vat_complete.bgz.tsv.gz vat_complete.bgz.tsv.gz\n",
    "\n",
    "python -c \"import pandas as pd; import numpy as np; import tqdm;\n",
    "worker_index=6\n",
    "WORKSPACE_BUCKET=${WORKSPACE_BUCKET}\n",
    "header = pd.read_csv('vat_complete.bgz.tsv.gz', nrows = 0, sep = '\\t', dtype = str).columns.tolist();\n",
    "transcript_specific_columns = [\n",
    "    'transcript',\n",
    "    'gene_symbol',\n",
    "    'transcript_source',\n",
    "    'aa_change',\n",
    "    'consequence',\n",
    "    'dna_change_in_transcript',\n",
    "    'exon_number',\n",
    "    'intron_number',\n",
    "    'gene_id',\n",
    "    'is_canonical_transcript',\n",
    "    \n",
    "];\n",
    "relevant_columns = ['vid',\n",
    " 'transcript',\n",
    " 'contig',\n",
    " 'position',\n",
    " 'ref_allele',\n",
    " 'alt_allele',\n",
    " 'gvs_afr_ac',\n",
    " 'gvs_afr_an',\n",
    " 'gvs_afr_af',\n",
    " 'gvs_eur_ac',\n",
    " 'gvs_eur_an',\n",
    " 'gvs_eur_af',\n",
    " 'gene_symbol',\n",
    " 'transcript_source',\n",
    " 'aa_change',\n",
    " 'consequence',\n",
    " 'dna_change_in_transcript',\n",
    " 'variant_type',\n",
    " 'exon_number',\n",
    " 'intron_number',\n",
    " 'genomic_location',\n",
    " 'dbsnp_rsid',\n",
    " 'gene_id',\n",
    " 'gene_omim_id',\n",
    " 'is_canonical_transcript',\n",
    " 'revel',\n",
    " 'splice_ai_acceptor_gain_score',\n",
    " 'splice_ai_acceptor_gain_distance',\n",
    " 'splice_ai_acceptor_loss_score',\n",
    " 'splice_ai_acceptor_loss_distance',\n",
    " 'splice_ai_donor_gain_score',\n",
    " 'splice_ai_donor_gain_distance',\n",
    " 'splice_ai_donor_loss_score',\n",
    " 'splice_ai_donor_loss_distance',\n",
    " 'omim_phenotypes_id',\n",
    " 'omim_phenotypes_name',\n",
    " 'clinvar_classification',\n",
    " 'clinvar_last_updated',\n",
    " 'clinvar_phenotype'];\n",
    "relevant_column_indices = np.flatnonzero([c in relevant_columns for c in header]).tolist()\n",
    "non_specific_columns = [c for c in relevant_columns if c not in transcript_specific_columns]\n",
    "\n",
    "def compress_df_chunk(df, extra_columns = None):\n",
    "    if extra_columns is not None:\n",
    "        non_specific_columns_other_than_vid = [c for c in non_specific_columns if c != 'vid'] + extra_columns\n",
    "    else:\n",
    "        non_specific_columns_other_than_vid = [c for c in non_specific_columns if c != 'vid']\n",
    "    non_specific_component = df.iloc[0,][non_specific_columns_other_than_vid]\n",
    "    transcript_specific_component = df[transcript_specific_columns].astype(str).agg(','.join)\n",
    "    return pd.concat([non_specific_component, transcript_specific_component])\n",
    "\n",
    "def gsutil_cp(src, dst, clobber = True, quiet = False):\n",
    "    import subprocess\n",
    "    \n",
    "    gsutil_options = ['-m']\n",
    "    if quiet:\n",
    "        gsutil_options += ['-q']\n",
    "    \n",
    "    cp_options = []\n",
    "    if not clobber:\n",
    "        cp_options += ['-n']\n",
    "    \n",
    "    subprocess.run(\n",
    "        ['gsutil'] + gsutil_options + ['cp'] + cp_options + [src, dst],\n",
    "        check = True,\n",
    "        universal_newlines = True\n",
    "    )\n",
    "\n",
    "dtypes = {c : str for c in relevant_columns}\n",
    "dtypes['gvs_afr_ac'] = int\n",
    "dtypes['gvs_afr_an'] = int\n",
    "dtypes['gvs_afr_af'] = float \n",
    "dtypes['gvs_eur_ac'] = int\n",
    "dtypes['gvs_eur_an'] = int\n",
    "dtypes['gvs_eur_af'] = float\n",
    "vat_iterator = pd.read_csv(\n",
    "    'vat_complete.bgz.tsv.gz', \n",
    "    chunksize = int(1e6), \n",
    "    sep = '\\t', \n",
    "    header = 0,\n",
    "    names = relevant_columns,\n",
    "    dtype = dtypes,\n",
    "    usecols = relevant_column_indices,\n",
    "    nrows = int(2e8),\n",
    "    skiprows = int(4.69e9)   \n",
    ")\n",
    "n_afr = 79826\n",
    "n_eur = 223350\n",
    "\n",
    "allele_number_threshold_afr = n_afr * 2 * .95\n",
    "allele_number_threshold_eur = n_eur * 2 * .95\n",
    "\n",
    "old_chromosomes = None\n",
    "relevant_chromosomes = [f'chr{i}' for i in range(1,23)]\n",
    "late_chromosomes = ['chr21', 'chr22']\n",
    "latest_chromosome = 'chr22'\n",
    "for chunk in tqdm.tqdm(vat_iterator):\n",
    "    chunk = chunk.query('contig in @late_chromosomes')\n",
    "    if chunk.shape[0] == 0:\n",
    "        break\n",
    "    chunk = chunk.query('contig == @latest_chromosome')\n",
    "    if chunk.shape[0] == 0:\n",
    "        continue\n",
    "    chunk_filtered = chunk.assign(\n",
    "        gvs_afr_an = lambda df: df.gvs_afr_an.astype(int),\n",
    "        gvs_eur_an = lambda df: df.gvs_eur_an.astype(int),\n",
    "        gvs_afr_af = lambda df: df.gvs_afr_af.astype(float),\n",
    "        gvs_eur_af = lambda df: df.gvs_eur_af.astype(float),\n",
    "    ).query(\n",
    "        '(0.001 <= gvs_afr_af <= .999) | (0.001 <= gvs_eur_af <= .999)'\n",
    "    ).query(\n",
    "        'gvs_afr_an >= @allele_number_threshold_afr'\n",
    "    ).query(\n",
    "        'gvs_eur_an >= @allele_number_threshold_eur'\n",
    "    )\n",
    "    chunk_filtered_collapsed = chunk_filtered.groupby(\n",
    "        ['vid'], sort = False\n",
    "    ).apply(\n",
    "        compress_df_chunk\n",
    "    ).reset_index(\n",
    "    ).assign(\n",
    "        CHR = lambda df: df.contig.str.slice(3)\n",
    "    )\n",
    "    new_chromosomes = chunk_filtered_collapsed.CHR.drop_duplicates().tolist()\n",
    "    if old_chromosomes is None:\n",
    "        old_chromosomes = new_chromosomes\n",
    "    for chromosome in new_chromosomes:\n",
    "        chunk_filtered_collapsed.query(\n",
    "            'CHR == @chromosome'\n",
    "        ).to_csv(\n",
    "            f'vat_chr{chromosome}_worker_{worker_index}.tsv',\n",
    "            mode = 'a',\n",
    "            header = False,\n",
    "            index = False,\n",
    "            sep = '\\t'\n",
    "        )\n",
    "    for chromosome in old_chromosomes:\n",
    "        if chromosome not in new_chromosomes:\n",
    "            gsutil_cp(f'vat_chr{chromosome}_worker_{worker_index}.tsv', f'{WORKSPACE_BUCKET}/data/vat/vat_chr{chromosome}_worker_{worker_index}.tsv')\n",
    "    old_chromosomes = new_chromosomes\"\n",
    "\n",
    "gsutil -m cp vat_chr* ${WORKSPACE_BUCKET}/data/vat/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%bash --out JOB_NAME\n",
    "source ~/aou_dsub.bash # This file was created via notebook 01_dsub_setup.ipynb.\n",
    "                \n",
    "aou_dsub \\\n",
    "  --machine-type n1-highmem-2 \\\n",
    "  --image ${DOCKER_IMAGE} \\\n",
    "  --env WORKSPACE_BUCKET ${WORKSPACE_BUCKET} \\\n",
    "  --disk-size 1000 \\\n",
    "  --script parse_vat_22.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process VATs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil rsync ${WORKSPACE_BUCKET}/data/vat /home/jupyter/data/vat/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head /home/jupyter/data/vat/vat_chr22_worker_6.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = pd.read_csv('/home/jupyter/data/vat/vat_complete.bgz.tsv.gz', nrows = 0, sep = '\\t', dtype = str).columns.tolist();\n",
    "transcript_specific_columns = [\n",
    "    'transcript',\n",
    "    'gene_symbol',\n",
    "    'transcript_source',\n",
    "    'aa_change',\n",
    "    'consequence',\n",
    "    'dna_change_in_transcript',\n",
    "    'exon_number',\n",
    "    'intron_number',\n",
    "    'gene_id',\n",
    "    'is_canonical_transcript',\n",
    "    \n",
    "];\n",
    "relevant_columns = ['vid',\n",
    " 'transcript',\n",
    " 'contig',\n",
    " 'position',\n",
    " 'ref_allele',\n",
    " 'alt_allele',\n",
    " 'gvs_afr_ac',\n",
    " 'gvs_afr_an',\n",
    " 'gvs_afr_af',\n",
    " 'gvs_eur_ac',\n",
    " 'gvs_eur_an',\n",
    " 'gvs_eur_af',\n",
    " 'gene_symbol',\n",
    " 'transcript_source',\n",
    " 'aa_change',\n",
    " 'consequence',\n",
    " 'dna_change_in_transcript',\n",
    " 'variant_type',\n",
    " 'exon_number',\n",
    " 'intron_number',\n",
    " 'genomic_location',\n",
    " 'dbsnp_rsid',\n",
    " 'gene_id',\n",
    " 'gene_omim_id',\n",
    " 'is_canonical_transcript',\n",
    " 'revel',\n",
    " 'splice_ai_acceptor_gain_score',\n",
    " 'splice_ai_acceptor_gain_distance',\n",
    " 'splice_ai_acceptor_loss_score',\n",
    " 'splice_ai_acceptor_loss_distance',\n",
    " 'splice_ai_donor_gain_score',\n",
    " 'splice_ai_donor_gain_distance',\n",
    " 'splice_ai_donor_loss_score',\n",
    " 'splice_ai_donor_loss_distance',\n",
    " 'omim_phenotypes_id',\n",
    " 'omim_phenotypes_name',\n",
    " 'clinvar_classification',\n",
    " 'clinvar_last_updated',\n",
    " 'clinvar_phenotype'];\n",
    "relevant_column_indices = np.flatnonzero([c in relevant_columns for c in header]).tolist()\n",
    "non_specific_columns = [c for c in relevant_columns if c not in transcript_specific_columns]\n",
    "non_specific_columns_other_than_vid = [c for c in non_specific_columns if c != 'vid']\n",
    "processed_header = ['vid'] + \\\n",
    "    non_specific_columns_other_than_vid + \\\n",
    "    transcript_specific_columns + \\\n",
    "    ['gvs_eur_missingness', 'gvs_afr_missingness', 'CHR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_df_chunk(df, extra_columns = None):\n",
    "    non_specific_columns_other_than_vid = [c for c in non_specific_columns if c != 'vid'] + extra_columns\n",
    "    non_specific_component = df.iloc[0,][non_specific_columns_other_than_vid]\n",
    "    transcript_specific_component = df[transcript_specific_columns].astype(str).agg(','.join)\n",
    "    return pd.concat([non_specific_component, transcript_specific_component])\n",
    "\n",
    "vat_dir = '/home/jupyter/data/vat/'\n",
    "unprocessed_vat_shards = [f for f in os.listdir(vat_dir) if '_worker_' in f]\n",
    "\n",
    "for chrom in tqdm.tqdm(list(range(1,22))):\n",
    "    \n",
    "    unprocessed_vat_shards_chrom = [f for f in unprocessed_vat_shards if f'chr{chrom}_' in f]\n",
    "    vat_shard_df_list = []\n",
    "    for f in unprocessed_vat_shards_chrom:\n",
    "        df_shard = pd.read_csv(\n",
    "            vat_dir + f,\n",
    "            sep = '\\t', \n",
    "            names = processed_header\n",
    "        )\n",
    "        df_shard.omim_phenotypes_id = df_shard.omim_phenotypes_id.astype(str)\n",
    "        df_shard.is_canonical_transcript = df_shard.is_canonical_transcript.astype(str)\n",
    "        vat_shard_df_list.append(df_shard)\n",
    "    vat_chrom = pd.concat(vat_shard_df_list)\n",
    "\n",
    "    duplicate_vids = vat_chrom[vat_chrom.vid.duplicated()].vid.tolist()\n",
    "    if len(duplicate_vids) > 0:\n",
    "        vat_chrom_duplicates = vat_chrom[[i in duplicate_vids for i in vat_chrom.vid]]\n",
    "        vat_chrom_duplicates_merged = vat_chrom_duplicates.groupby(\n",
    "            ['vid'], sort = False\n",
    "        ).apply(\n",
    "            lambda df: compress_df_chunk(df, extra_columns = ['CHR', 'gvs_eur_missingness', 'gvs_afr_missingness']),\n",
    "        ).reset_index(\n",
    "        )\n",
    "        vat_chrom_deduped = pd.concat([\n",
    "            vat_chrom[[i not in duplicate_vids for i in vat_chrom.vid]], \n",
    "            vat_chrom_duplicates_merged\n",
    "        ]).sort_values(\n",
    "            ['position', 'ref_allele', 'alt_allele']\n",
    "        )\n",
    "    else:\n",
    "        vat_chrom_deduped = vat_chrom\n",
    "    \n",
    "    vat_chrom_deduped.to_parquet(\n",
    "        vat_dir + f'vat_chr{chrom}.parquet'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = pd.read_csv('/home/jupyter/data/vat/vat_complete.bgz.tsv.gz', nrows = 0, sep = '\\t', dtype = str).columns.tolist();\n",
    "transcript_specific_columns = [\n",
    "    'transcript',\n",
    "    'gene_symbol',\n",
    "    'transcript_source',\n",
    "    'aa_change',\n",
    "    'consequence',\n",
    "    'dna_change_in_transcript',\n",
    "    'exon_number',\n",
    "    'intron_number',\n",
    "    'gene_id',\n",
    "    'is_canonical_transcript',\n",
    "    \n",
    "];\n",
    "relevant_columns = ['vid',\n",
    " 'transcript',\n",
    " 'contig',\n",
    " 'position',\n",
    " 'ref_allele',\n",
    " 'alt_allele',\n",
    " 'gvs_afr_ac',\n",
    " 'gvs_afr_an',\n",
    " 'gvs_afr_af',\n",
    " 'gvs_eur_ac',\n",
    " 'gvs_eur_an',\n",
    " 'gvs_eur_af',\n",
    " 'gene_symbol',\n",
    " 'transcript_source',\n",
    " 'aa_change',\n",
    " 'consequence',\n",
    " 'dna_change_in_transcript',\n",
    " 'variant_type',\n",
    " 'exon_number',\n",
    " 'intron_number',\n",
    " 'genomic_location',\n",
    " 'dbsnp_rsid',\n",
    " 'gene_id',\n",
    " 'gene_omim_id',\n",
    " 'is_canonical_transcript',\n",
    " 'revel',\n",
    " 'splice_ai_acceptor_gain_score',\n",
    " 'splice_ai_acceptor_gain_distance',\n",
    " 'splice_ai_acceptor_loss_score',\n",
    " 'splice_ai_acceptor_loss_distance',\n",
    " 'splice_ai_donor_gain_score',\n",
    " 'splice_ai_donor_gain_distance',\n",
    " 'splice_ai_donor_loss_score',\n",
    " 'splice_ai_donor_loss_distance',\n",
    " 'omim_phenotypes_id',\n",
    " 'omim_phenotypes_name',\n",
    " 'clinvar_classification',\n",
    " 'clinvar_last_updated',\n",
    " 'clinvar_phenotype'];\n",
    "relevant_column_indices = np.flatnonzero([c in relevant_columns for c in header]).tolist()\n",
    "non_specific_columns = [c for c in relevant_columns if c not in transcript_specific_columns]\n",
    "non_specific_columns_other_than_vid = [c for c in non_specific_columns if c != 'vid']\n",
    "processed_header = ['vid'] + \\\n",
    "    non_specific_columns_other_than_vid + \\\n",
    "    transcript_specific_columns + \\\n",
    "    ['CHR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_df_chunk(df, extra_columns = None):\n",
    "    non_specific_columns_other_than_vid = [c for c in non_specific_columns if c != 'vid'] + extra_columns\n",
    "    non_specific_component = df.iloc[0,][non_specific_columns_other_than_vid]\n",
    "    transcript_specific_component = df[transcript_specific_columns].astype(str).agg(','.join)\n",
    "    return pd.concat([non_specific_component, transcript_specific_component])\n",
    "\n",
    "vat_dir = '/home/jupyter/data/vat/'\n",
    "unprocessed_vat_shards = [f for f in os.listdir(vat_dir) if '_worker_' in f]\n",
    "\n",
    "for chrom in tqdm.tqdm(list(range(22,23))):\n",
    "    \n",
    "    unprocessed_vat_shards_chrom = [f for f in unprocessed_vat_shards if f'chr{chrom}_' in f]\n",
    "    vat_shard_df_list = []\n",
    "    for f in unprocessed_vat_shards_chrom:\n",
    "        df_shard = pd.read_csv(\n",
    "            vat_dir + f,\n",
    "            sep = '\\t', \n",
    "            names = processed_header\n",
    "        )\n",
    "        df_shard.omim_phenotypes_id = df_shard.omim_phenotypes_id.astype(str)\n",
    "        df_shard.is_canonical_transcript = df_shard.is_canonical_transcript.astype(str)\n",
    "        vat_shard_df_list.append(df_shard)\n",
    "    vat_chrom = pd.concat(vat_shard_df_list)\n",
    "\n",
    "    duplicate_vids = vat_chrom[vat_chrom.vid.duplicated()].vid.tolist()\n",
    "    if len(duplicate_vids) > 0:\n",
    "        vat_chrom_duplicates = vat_chrom[[i in duplicate_vids for i in vat_chrom.vid]]\n",
    "        vat_chrom_duplicates_merged = vat_chrom_duplicates.groupby(\n",
    "            ['vid'], sort = False\n",
    "        ).apply(\n",
    "            lambda df: compress_df_chunk(df, extra_columns = ['CHR']),\n",
    "        ).reset_index(\n",
    "        )\n",
    "        vat_chrom_deduped = pd.concat([\n",
    "            vat_chrom[[i not in duplicate_vids for i in vat_chrom.vid]], \n",
    "            vat_chrom_duplicates_merged\n",
    "        ]).sort_values(\n",
    "            ['position', 'ref_allele', 'alt_allele']\n",
    "        )\n",
    "    else:\n",
    "        vat_chrom_deduped = vat_chrom\n",
    "    \n",
    "    vat_chrom_deduped.to_parquet(\n",
    "        vat_dir + f'vat_chr{chrom}.parquet'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make non-synonymous annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonymous_classes = ['synonymous_variant', 'start_retained_variant', 'stop_retained_variant']\n",
    "nonsynonymous_classes = ['frameshift_variant', 'inframe_deletion', 'inframe_insertion', 'missense_variant', \n",
    "                         'start_lost', 'stop_gained', 'stop_lost']\n",
    "for chr in tqdm.tqdm(range(1,23)):\n",
    "    vat_chr = pd.read_parquet(f'/home/jupyter/data/vat/vat_chr{chr}.parquet')\n",
    "    vat_chr_with_consequence = vat_chr.dropna(subset = ['consequence'])\n",
    "    vat_chr_nonsynonymous = vat_chr_with_consequence[reduce(lambda a, b: a | b,  [vat_chr_with_consequence.consequence.str.contains(c) for c in nonsynonymous_classes])]\n",
    "    vat_chr_synonymous = vat_chr_with_consequence[reduce(lambda a, b: a | b,  [vat_chr_with_consequence.consequence.str.contains(c) for c in synonymous_classes])]\n",
    "    nonsynonymous_vids = set(vat_chr_nonsynonymous.vid.str.replace('-', ':').tolist())\n",
    "    synonymous_vids = set(vat_chr_synonymous.vid.str.replace('-', ':').tolist())\n",
    "    synonymous_excluding_nonsynonymous_vids = synonymous_vids.difference(nonsynonymous_vids)\n",
    "    bim = pd.read_csv(\n",
    "        f'/home/jupyter/data/plink/chr{chr}_afr70346.bim',\n",
    "        sep = '\\t',\n",
    "        names = ['CHR', 'SNP', 'CM', 'BP', 'A1', 'A2']\n",
    "    ).assign(\n",
    "        synonymous = lambda df: np.array([v in synonymous_excluding_nonsynonymous_vids for v in df.SNP]).astype(int),\n",
    "        non_synonymous = lambda df: np.array([v in nonsynonymous_vids for v in df.SNP]).astype(int),\n",
    "    )\n",
    "    bim.to_csv(\n",
    "        f'/home/jupyter/data/annotations/synonymous_and_nonsynonymous/chr{chr}_afr70346.annot.gz',\n",
    "        index = False,\n",
    "        sep = '\\t'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
